{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f6b7a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement elasticsearch-helpers (from versions: none)\n",
      "ERROR: No matching distribution found for elasticsearch-helpers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: elasticsearch in c:\\users\\shubham\\appdata\\roaming\\python\\python310\\site-packages (8.8.2)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in c:\\users\\shubham\\appdata\\roaming\\python\\python310\\site-packages (from elasticsearch) (8.4.0)\n",
      "Requirement already satisfied: urllib3<2,>=1.26.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.14)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from elastic-transport<9,>=8->elasticsearch) (2022.12.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kafka-python in c:\\users\\shubham\\appdata\\roaming\\python\\python310\\site-packages (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch-helpers\n",
    "!pip install elasticsearch\n",
    "!pip install kafka-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092c2f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kafka-python in c:\\users\\shubham\\appdata\\roaming\\python\\python310\\site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade kafka-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8ec2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.4.1.tar.gz (310.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7\n",
      "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285432 sha256=240d14dcb3d38dbcb2d5a4bd3dbcbe0d25faacca536c3f57535abdacc29a1387\n",
      "  Stored in directory: c:\\users\\shubham\\appdata\\local\\pip\\cache\\wheels\\53\\fe\\23\\517784b9d9dadfb82c5676e76483422096aa5dc20d4d602213\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.4.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyspark\n",
    "#import pysaprk\n",
    "\n",
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.appName('demo1').getOrCreate()\n",
    "#spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f912b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from kafka.errors import KafkaError\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d641ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configuration\n",
    "kafka_bootstrap_servers = 'localhost:9092'\n",
    "kafka_topic = 'clickstream_topic'\n",
    "\n",
    "# Elasticsearch configuration\n",
    "es_host = 'localhost'\n",
    "es_port = 9200\n",
    "es_index = 'clickstream_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0123df0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3439119573.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[15], line 41\u001b[1;36m\u001b[0m\n\u001b[1;33m    if len(data_store) >= 100:  # Process data when a certain threshold is reached\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Set up Kafka consumer\n",
    "consumer = KafkaConsumer(kafka_topic, bootstrap_servers=kafka_bootstrap_servers)\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch([{'host': es_host, 'port': es_port}])\n",
    "\n",
    "# Data store (you can choose your preferred data store)\n",
    "data_store = {}  # Dictionary to store clickstream data\n",
    "\n",
    "# Process and index the data\n",
    "for message in consumer:\n",
    "    try:\n",
    "        click_data = json.loads(message.value)\n",
    "        click_id = click_data['click_id']\n",
    "        user_id = click_data['user_id']\n",
    "        timestamp = click_data['timestamp']\n",
    "        url = click_data['url']\n",
    "        country = click_data['country']\n",
    "        city = click_data['city']\n",
    "        browser = click_data['browser']\n",
    "        os = click_data['os']\n",
    "        device = click_data['device']\n",
    "        \n",
    "        data_store[click_id] = {\n",
    "            'click_data': {\n",
    "                'user_id': user_id,\n",
    "                'timestamp': timestamp,\n",
    "                'url': url\n",
    "            },\n",
    "            'geo_data': {\n",
    "                'country': country,\n",
    "                'city': city\n",
    "            },\n",
    "            'user_agent_data': {\n",
    "                'browser': browser,\n",
    "                'os': os,\n",
    "                'device': device\n",
    "            },\n",
    "        }\n",
    "        \n",
    "         if len(data_store) >= 100:  # Process data when a certain threshold is reached\n",
    "            processed_data = []\n",
    "            for click_id, data in data_store.items():\n",
    "                # Perform data processing/aggregation by URL and country\n",
    "                url = data['click_data']['url']\n",
    "                country = data['geo_data']['country']\n",
    "                timestamp = datetime.strptime(data['click_data']['timestamp'], '%Y-%m-%d %H:%M:%S')\n",
    "                # ... perform other calculations as needed\n",
    "\n",
    "                # Create a processed data document\n",
    "                processed_doc = {\n",
    "                    'url': url,\n",
    "                    'country': country,\n",
    "                    'timestamp': timestamp,\n",
    "                    # ... add other calculated fields\n",
    "                }\n",
    "                processed_data.append(processed_doc)\n",
    "\n",
    "            # Index the processed data in Elasticsearch\n",
    "            bulk_data = [\n",
    "                {\n",
    "                    '_index': es_index,\n",
    "                    '_source': doc\n",
    "                }\n",
    "                for doc in processed_data\n",
    "            ]\n",
    "            bulk(es, bulk_data)  # Bulk index the processed data\n",
    "\n",
    "            # Clear the data store after processing\n",
    "            data_store.clear()\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON format\")\n",
    "    except KafkaError as e:\n",
    "        print(f\"Kafka error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865ad66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
